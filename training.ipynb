{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training Notebook\n",
    "This notebook encompasses model training.\n",
    "This notebook uses Tensorflow.\n",
    "\n",
    "Be careful, all the libraries I use a very sensitive to different versions of each other. Here are the versions of the packages I used:\n",
    "- Python == 3.10.10\n",
    "- tensorflow == 2.11.0\n",
    "- tensorflow-io == 0.31.0\n",
    "- Numpy == 1.23.5\n",
    "- Matplotlib == 3.7.1\n",
    "- Jupyter Notebook == 6.5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fix parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from model import *\n",
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "# - Modify the parameters at will -------------------------\n",
    "nb_samples = 300            # Total number of samples in the dataset\n",
    "key = \"P\"                # The key used for generating the data. Datapoints npy files should have the format \"key_i.npy\" with i the index ranging from 0 to nb_samples.\n",
    "dataset_dir = \"dataset_vector_{}\".format(key)\n",
    "duration = 45               # In seconds, the duration of the total audio segment\n",
    "\n",
    "nb_epochs = 45              # Number of epochs for the duration of the training\n",
    "training_index = 1         # Index to differentiate training from the others in the logs\n",
    "weight_save_frequency = 1   # In epochs, save frequency for the model weights\n",
    "\n",
    "batch_size = 32\n",
    "latent_dim = 64             # Number of dimensions of the latent spce\n",
    "cut = int(0.8*nb_samples)   # Defines the train/validation cut. NB: A good cut is generally 80/20 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## I. Prepare callbacks and dataset for training. Load and build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data points:  (2504, 48)\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Model: \"autoencoder_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_1 (Encoder)         multiple                  6445568   \n",
      "                                                                 \n",
      " decoder_1 (Decoder)         multiple                  7905089   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,350,657\n",
      "Trainable params: 14,350,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# -- Dataset --------------------------------------\n",
    "first = np.load(\"{}/{}_0.npy\".format(dataset_dir, key))  # Load the first datapoint to retrieve the shape\n",
    "data_shape = first.shape    # Shape for the whole dataset\n",
    "print(\"Shape of the data points: \", data_shape)\n",
    "\n",
    "time_stamps = np.load(\"miscellaneous/time_stamps_{}.npy\".format(key))\n",
    "spec_indices = np.load(\"miscellaneous/spectrogram_indices_{}.npy\".format(key))\n",
    "\n",
    "dataset = np.zeros((nb_samples, data_shape[0], data_shape[1]))  # Create the empty vector that will hold the full data\n",
    "dataset[0] = first\n",
    "\n",
    "for i in range(1, nb_samples):\n",
    "    dataset[i] = np.load(\"{}/{}_{}.npy\".format(dataset_dir, key, i))\n",
    "\n",
    "dataset = dataset.reshape((nb_samples, data_shape[0], data_shape[1], 1))\n",
    "\n",
    "\n",
    "# -- Callbacks -------------------------------------\n",
    "checkpoint_path = \"logs/training_{training_index}/cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1,\n",
    "                                                 save_freq=\"epoch\")\n",
    "\n",
    "%load_ext tensorboard\n",
    "import datetime\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_training_{training_index}\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=weight_save_frequency)\n",
    "\n",
    "\n",
    "# -- Build the Model --------------------------------\n",
    "autoencoder = Autoencoder(data_shape, latent_dim)\n",
    "autoencoder.compile(optimizer=\"adam\",\n",
    "                    loss=losses.MeanSquaredError())\n",
    "autoencoder.build((None, data_shape[0], data_shape[1], 1))\n",
    "\n",
    "autoencoder.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## II. Fit the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "autoencoder.fit(dataset[:cut],\n",
    "                dataset[:cut],\n",
    "                epochs=nb_epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(dataset[cut:], dataset[cut:]), callbacks=[tensorboard_callback, cp_callback])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## III. Visualize training in Tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#%reload_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}